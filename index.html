
<html>
<head>
<title>Shiyan Yang, Senior Research Scientist, Seeing Machines</title>
<meta name="keywords" content="shiyan yang, research, human factors, machine learning, driver monitoring systems">
<meta name="description" content="Shiyan Yang's home page">
<meta charset="utf-8"> 
</head>



<body bgcolor="#ffffff" text="#000000" link="#000000" vlink="#000000" alink="#000000">
<font face="Georgia">

<p align="center">
<table width="60%" border="0" cellspacing="1" cellpadding="30">
<tr><td>


<style>
  a:link {
    color:#2B65EC;
    background-color: transparent;
    text-decoration: underline;
  }
  a:hover {
  color: #157DEC;
  background-color: transparent;
  text-decoration:  underline;
  }
</style>


  
<img src="https://github.com/ShiyanYang/shiyanyang.github.io/blob/main/img/Shiyan_Yang_profile2.jpg?raw=true" alt="Shiyan Yang Profile" width="175")>
<p style="font-size:24px;">
  <b>Yang, Shiyan  杨世炎</b>
</p>

<p>
Scientist, Melbourne, Australia<br>
</p>

<section>
<p><b>About Me</b></p>
  <p>
    The goal of my career is to empower machines and AI to better understand and support humans. Specifically, my research focuses on integrating human factors—behavioral, cognitive, and physiological—into machine learning and AI systems to enhance user safety, performance, and overall experience.
</p>

  <p>This vision inspired me to join Seeing Machines (2017-2024), where I contributed to the innovation of driver monitoring systems, a groundbreaking technology at the time aimed at saving lives on the road. By combining human factors expertise with machine learning, I advanced the detection of driver distraction, drowsiness, and the transition of control in complex driving scenarios.
</p>
  
<p>My research has been instrumental in shaping the technical roadmaps, algorithm development, and industry guidelines for automotive-grade driver monitoring systems. It has also garnered recognition through publications in top-tier journals and prestigious international awards, such as Patricia F. Waller Award.
  </p>
</section>

<div style="height:40px;"></div> <!-- Adds vertical space between sections -->


<section>
<p><b>My Story of Integrating Human Factors into Machine Learning for Driver State Monitoring</b></p>
<p>
    Over the past decade, driver monitoring systems have evolved from lab prototypes to automotive-grade products that save lives on the road. I was fortunate to contribute to this breakthrough, integrating human factors into machine learning for real-world impact.
</p>

<p>Texas A&M University (2010-2016): Where My Journey Began<p>
<p>
  My journey started with my doctoral research on human factors and cognitive systems, where I explored novel metrics to enhance human performance in complex environments. Under Thomas Ferris’ supervision, I proposed “cognitive efficiency” - a metric quantifying how much information a person processes per “unit” of cognitive resources. A core chapter of my thesis involved measuring cognitive workload using physiological sensors. To deepen this research, I took graduate courses across multiple areas - computer science, statistics, psychology, and kinesiology - which fueled my curiosity about interdisciplinary connections. In my final year, a paper from Birsen Donmez’s lab, “Smart driver monitoring: When signal processing meets human factors”, opened my eyes to an exciting future: the intersection of human factors and machine learning to create intelligent human sensing.
</p>

<p>UC Berkeley (2016-2017): A Brief but Pivotal Chapter</p>
<p>
  As a postdoctoral scholar, I collaborated with Steven Shladover, a pioneer of intelligent transportation systems, to study driver-automation interaction in truck platooning. It was an unforgettable experience of operating a three-truck platooning on Bay Area freeways. But Berkeley offered even more. During the kickoff meeting of the NSF VeHiCal project, I met Ruzena Bajcsy, a renowned Computer Science professor, who emphasized the importance of human factors for human sensing. Surrounded by several high-profile Berkeley professors, I was, however, too nervous to contribute - yet that moment profoundly shaped my vision of connecting human factors with machine learning. 
</p>

<p>
  By 2017, human factors was mostly framed as UI/UX in Silicon Valley, which didn’t fully aligned with my goal. Then, an unexpected opportunity arose on the other side of the world - Seeing Machines, an Australian company, was transitioning from eye tracking to driver monitoring. It needed human factors experts to help computer scientists understand driver state and behavior. After interviewing with Mike Lenne, their Human Factors Lead, and consulting Steve and Tom, I took a leap of faith - moving to Australia to embrace a new adventure. 
</p>

<p>Seeing Machines (2017-2024): Where Research Met Real-World Impact</p>
<p>
  Joining Seeing Machines in its ascent, I led the CANdrive Automated vehicle trial (AU$1.35 millions) and later the collaboration with the MIT Advanced Vehicle Technology consortium (founded by Bryan Reimer). I initiated cutting-edge research on distraction, drowsiness, takeover readiness, and gaze-context analysis. These research demonstrated the unique value of human factors in defining ground truth, curating high-quality data, and enhancing feature engineering for driver state detection. Moreover, I explored computation models - from probabilistic machine learning to deep learning to generative AI - to analyze driver behavior and dynamic contexts. The findings offered novel and holistic insights into driver-automation-environment interactions, shaping the future of driver state monitoring.
</p>

<p>The Next Chapter</p>
<p>
  Every journey has its end. At the end of 2024, my time at Seeing Machines came to an end due to the volatility of the automative industry. I knew this also meant the end of my career as an automotive human factors researcher - Australia doesn’t build cars. However, this also means a new start, towards a broader future of empowering AI to support human tasks. I hope my story will inspire others who are passionated about human factors and AI for real-world influence.
</p>
</section>

<div style="height:40px;"></div> <!-- Adds vertical space between sections -->

<section>
<p><b>Research Projects and Publications</b></p>
<p>2017-2024, Seeing Machines
    <ul>  
      <li> <a href="https://agelab.mit.edu/avt" target="_blank">MIT Advanced Vehicle Technology Consortium</a>
        <p style="color:grey">
          Yang, S., McKerral, A., Mulhall, M. D., Lenn&#233, M. G., Reimer, B., & Gershon, P. (2023). 
         <a href="https://dl.acm.org/doi/pdf/10.1145/3580585.3606459 " target="_blank">
        Takeover Context Matters: Characterising Context of Takeovers in Naturalistic Driving using Super Cruise and Autopilot.
        </a>
        In Proceedings of the 15th International Conference on Automotive User Interfaces and Interactive Vehicular Applications, 112-122.
        </p>
        
        <p style="color:grey">
        Yang, S., Lenn&#233, M.G., Reimer, B., Gershon, P. (2022). 
        Modeling Driver-Automation Interaction using A Naturalistic Multimodal Driving Dataset. 
        In Proceedings of the 66th Human Factors & Ergonomics Society International Annual Meeting,66(1), 1462-1466.
        </p>

      <li> <a href="https://seeingmachines.com/advanced-safe-truck-concept-launch/" target="_blank">Advanced Safe Truck Concept (ASTC)</a>
        <p style="color:grey">
          Yang, S., Kuo, J., Lenn&#233, M. G., Fitzharris, M., Horberry, T., Blay, K., &#8230 and Truche, C. (2021). 
          The impacts of temporal variation and individual differences in driver cognitive workload on ECG-based detection. 
          <i>Human Factors, 63</i>(5), 2021. 
          
        </p>

      <li> <a href="https://seeingmachines.com/can-drive-phase-1/" target="_blank">CANdrive Automated Vehicle Trial</a>
        <p style="color:grey">
          Yang, S., Wilson, K.M., Shiferaw, B., Trey, R., Kuo, J., and Lenn&#233, M.G. (2024). 
          Sensor fusion to connect gaze fixation with dynamic driving context for driver attention management. 
          <i>Transportation Research Part F, (107)</i>578-588.
        </p>
        
        <p style="color:grey">
          Yang, S., Wilson, K.M., Trey, R., Kuo, J., and Lenn&#233, M.G. (2022). 
          Beyond gaze fixation: Modeling peripheral vision in relation to speed, Tesla Autopilot, cognitive load, and age in highway driving. 
          <i>Accident Analysis & Prevention, 177</i>, 106670.
        </p>
        
        <p style="color:grey">
          Yang, S., Wilson, K.M., Trey, R., Kuo, J., and Lenn&#233, M.G. (2022). 
          Evaluating driver features for cognitive distraction detection and validation in manual and Level 2 automated driving. 
          <i>Human Factors, 64</i>(4), 746-759.
        </p>

        <p style="color:grey">
          Yang, S., Kuo, J., and Lenn&#233, M.G. (2021). 
          Effects of distraction in on-road level-2 automated driving: Impacts on glance behavior and take-over performance. 
          <i>Human Factors, 63</i>(8), 1485-1497.
        </p>
        
        <p style="color:grey">
          Wilson, K. M., Yang, S., Roady, T., Kuo, J., and Lenn&#233, M. G. (2020). 
          Driver trust and mode confusion in an on-road study of automated vehicle technology. 
          <i>Safety Science, 130</i>, 104845.
        </p>
    </ul>
<p>2016-2017, PATH, The University of California, Berkeley</p>
    <ul> 
      <li> <a href="https://path.berkeley.edu/research/connected-and-automated-vehicles/truck-platooning" target="_blank">Connected and Automated Truck Platooning</a>
        <p style="color:grey">
          Yang, S., Shladover, S.E., Lu, X., Ramezani, H., Kailas, A., and Altan, O.D. (2021). 
          A Bayesian regression analysis of truck drivers&#39 use of cooperative adaptive cruise control (CACC) for platooning on California highways. 
          <i>Journal of Intelligent Transportation Systems</i>, 1-12.
        </p>

        <p style="color:grey">
          Yang, S., Shladover, S.E., Lu, X., Ramezani, H., Kailas, A., and Altan, O.D. (2018). 
          A first investigation of truck drivers&#39 preferences and behaviors using a prototype cooperative adaptive cruise control system.  
          <i>Transportation Research Record, 2672</i>(2), 39-48.
        </p>
    </ul>
<p>2010-2016, Industrial Engineering, Texas A&M University</p>
    <ul> 
      <li> Multimodal Display Design
           <p style="color:grey">Yang, S., & Ferris, T. K. (2019). Supporting Multitracking Performance with Novel Visual, Auditory, and Tactile Displays. 
             <i>IEEE Transactions on Human-Machine Systems, 50</i>(1), 79-88.
           </p>
           
      <li> Cognitive Efficiency in Human-Machine Systems
          <p style="color:grey">Yang, S., & Ferris, T. K. (2018). Cognitive efficiency in human-machine systems: 
            Metrics of display effectiveness for supporting multitask performance. 
            <i>Journal of Cognitive Engineering and Decision Making, 12</i>(2), 153-169.
          </p>
    </ul>
</section>

<section>
<p><b>Awards</b></p>
  <ul>
    <li>Best Papaer Award, by the Road User Measurement and Evaluation Committee (ACH60), Transportation Research Board (TRB), 2023
    <li>Best Extended Abstract/Paper, Australasian Road Safety Conference, 2022
    <li><a href="http://www.trb.org/AboutTRB/WallerAward.aspx " target="_blank">Patricia F. Waller Award</a>, TRB, 2018
    <li><a href="https://www.ugpti.org/trb/truckandbus/deborahfreund.php/" target="_blank">Deborah Freund ACS60 Paper Award</a>, by TRB Truck and Bus Safety Committee, 2018
    <li>North American Automotive Innovation & Startup Competition Award, NACSAE, 2016
    <li>Alphonse Chapanis Best Student Paper Award (Finalist, top 3 of 37), the Human Factors and Ergonomics Society, 2014
  </ul>
</section>

<section>
  <p><b>Education</b></p>
    <ul>
      <li>PhD, Texas A&M University, 2016</a>
    </ul>
</section>



</font>
</body>
</html>


